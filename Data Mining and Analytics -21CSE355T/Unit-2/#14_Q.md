1. What is Market Basket Analysis? Explain its significance in data mining.

2. Define support and confidence in the context of association rules.

3. State the Apriori property and explain why it is used for pruning in the Apriori algorithm.

4. List the main steps of the Apriori algorithm for mining frequent itemsets.

5. Differentiate between frequent itemsets and closed frequent itemsets.

6. What is the difference between horizontal and vertical data formats in transaction databases?

7. Explain how frequent itemsets are mined using vertical data format (e.g., using intersection of TID lists).

8. What are multilevel association rules? Give an example.

9. Distinguish between multilevel and multidimensional association rules.

10. What is correlation analysis in association rule mining? Why is it needed beyond just support and confidence?

11. Explain constraint-based association mining with examples of constraints.

12. True or False: In Apriori, if an itemset is infrequent, all its supersets are also infrequent.

13. True or False: Multidimensional association rules involve attributes from multiple dimensions, such as age, income, and purchases.

14. Given minsup = 2 and the following transactions:  
T1: A B C  
T2: A B  
T3: B C D  
T4: A C D  
List all frequent 1-itemsets, 2-itemsets, and 3-itemsets.

15. Using the transactions from Q14, apply Apriori to find all frequent itemsets with minsup = 50% (assume 4 transactions).

16. For a rule X → Y, support(X → Y) = support(X ∪ Y), but confidence(X → Y) = support(X ∪ Y) / support(X). Explain the difference.

17. What is the role of candidate generation in Apriori? How does pruning reduce the number of candidates?

18. Explain the concept of mining frequent itemsets without candidate generation. Name one method (though not detailed in syllabus, relate to alternatives like FP-Growth).

19. What are closed frequent itemsets? Why are they useful?

20. Describe the process of mining closed frequent itemsets.

21. Give an example of multilevel association rules using product categories (e.g., electronics → computer → laptop).

22. In multidimensional association rules, how do quantitative attributes differ from categorical ones?

23. What is the lift measure in correlation analysis? How is it calculated?

24. If lift > 1 for a rule, what does it indicate?

25. Explain negative correlation in association rules.

26. List two types of constraints used in constraint-based mining (e.g., knowledge-based, data-driven).

27. Why is correlation analysis preferred over confidence alone for strong rules?

28. MCQ: Which algorithm uses candidate generation and level-wise search?  
   a) FP-Growth b) Apriori c) ECLAT d) Partition

29. MCQ: Vertical data format mining uses:  
   a) TID lists intersection b) Candidate pruning c) Tree structure d) Horizontal scans

30. MCQ: Multilevel association rules are mined at:  
   a) Single abstraction level b) Multiple levels of granularity c) Only top level d) Bottom level only

31. Apply Apriori on this dataset (minsup=3/6=50%):  
T1: Milk, Bread, Butter  
T2: Bread, Butter  
T3: Milk, Bread  
T4: Milk, Butter  
T5: Bread, Butter  
T6: Milk, Bread, Butter  
Find frequent itemsets.

32. From Q31, generate all strong association rules with minconf=70%.

33. What challenges arise when mining multilevel association rules?

34. Explain how vertical format improves efficiency over horizontal format in Apriori.

35. What is the ECLAT algorithm? How does it differ from Apriori?

36. Define mining multidimensional association rules with an example (e.g., age(X, young) ∧ occupation(X, student) → buys(X, laptop)).

37. How are quantitative multidimensional association rules handled (e.g., discretization)?

38. True or False: Constraint-based mining allows user-specified rules to guide the search.

39. Analyze why Apriori may generate many candidates in large datasets.

40. Explain the steps to mine frequent itemsets using vertical data format with an example TID list intersection.

41. What is the purpose of mining closed frequent itemsets instead of all frequent ones?

42. Give an example where correlation analysis reveals a misleading rule (high confidence but low lift).

43. MCQ: Which is NOT a measure for association rules?  
   a) Support b) Confidence c) Lift d) Accuracy (of classification)

44. In constraint-based mining, what is a succinct constraint? Give an example.

45. Explain the anti-monotone property in constraint-based association mining.

46. Compare multilevel and multidimensional association rules with respect to data structure.

47. If support({A,B})=0.4, support({A})=0.6, support({B})=0.7, calculate confidence(A→B) and lift(A→B).

48. Why is candidate generation expensive in Apriori? Suggest one improvement (e.g., vertical format or other).

49. Discuss applications of association rules in recommender systems or retail.

50. Analyze the advantages and limitations of constraint-based association rule mining.
